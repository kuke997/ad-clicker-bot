import aiohttp
import random
import asyncio
from datetime import datetime, timedelta
import logging
import re

class ProxyManager:
    def __init__(self):
        self.proxy_pool = []
        self.last_refresh = datetime.min
        self.logger = logging.getLogger("proxy_manager")
        self.proxy_score = {}
        self.failed_proxies = set()
    
    async def fetch_proxies(self):
        """ä½¿ç”¨æ›´å¯é çš„ä»£ç†æºï¼ŒåŒ…æ‹¬HTTPSä»£ç†"""
        sources = [
            "https://proxylist.geonode.com/api/proxy-list?protocols=http,https&limit=300&country=US,GB,CA,DE,FR",
            "https://api.proxyscrape.com/v2/?request=getproxies&protocol=http&timeout=10000&country=all&ssl=yes",
            "https://raw.githubusercontent.com/TheSpeedX/PROXY-List/master/http.txt",
            "https://raw.githubusercontent.com/jetkai/proxy-list/main/online-proxies/txt/proxies-https.txt",
            "https://raw.githubusercontent.com/roosterkid/openproxylist/main/HTTPS_RAW.txt"
        ]
        
        proxies = set()
        async with aiohttp.ClientSession() as session:
            for url in sources:
                try:
                    self.logger.info(f"è·å–ä»£ç†æº: {url}")
                    async with session.get(url, timeout=25) as response:
                        content_type = response.headers.get('Content-Type', '')
                        
                        if "application/json" in content_type:
                            data = await response.json()
                            if "geonode" in url:
                                for item in data["data"]:
                                    proxy = f"{item['ip']}:{item['port']}"
                                    proxies.add(proxy)
                            else:
                                # å¤„ç†å…¶ä»–JSONæ ¼å¼çš„ä»£ç†æº
                                for item in data:
                                    if 'ip' in item and 'port' in item:
                                        proxy = f"{item['ip']}:{item['port']}"
                                        proxies.add(proxy)
                        else:
                            text = await response.text()
                            for line in text.splitlines():
                                proxy = line.strip()
                                if re.match(r'^\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}:\d+$', proxy):
                                    proxies.add(proxy)
                except Exception as e:
                    self.logger.error(f"ä»£ç†æºè·å–å¤±è´¥ {url}: {str(e)}")
        
        self.logger.info(f"ä»æºè·å–åˆ° {len(proxies)} ä¸ªå€™é€‰ä»£ç†")
        return list(proxies)
    
    async def validate_proxy(self, proxy):
        """ä½¿ç”¨å¼‚æ­¥éªŒè¯ä»£ç†ï¼Œæµ‹è¯•HTTPå’ŒHTTPSè¿æ¥"""
        test_urls = [
            "http://www.example.com",
            "https://www.example.com",
            "http://www.google.com/gen_204",
            "https://www.wikipedia.org"
        ]
        
        async with aiohttp.ClientSession() as session:
            for url in test_urls:
                try:
                    start_time = datetime.now()
                    # å¢åŠ è¶…æ—¶æ—¶é—´åˆ°20ç§’
                    timeout = aiohttp.ClientTimeout(total=20)
                    
                    # æ ¹æ®URLåè®®ç¡®å®šä»£ç†åè®®
                    proxy_type = "https" if url.startswith("https") else "http"
                    
                    async with session.get(
                        url,
                        proxy=f"{proxy_type}://{proxy}",
                        timeout=timeout,
                        headers={"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36"},
                        ssl=False  # ç¦ç”¨SSLéªŒè¯ä»¥åŠ å¿«é€Ÿåº¦
                    ) as response:
                        if response.status in [200, 204]:
                            speed = (datetime.now() - start_time).total_seconds()
                            return True, speed
                except asyncio.TimeoutError:
                    self.logger.warning(f"âŒ› ä»£ç†éªŒè¯è¶…æ—¶: {proxy} | URL: {url}")
                except Exception as e:
                    continue
        
        self.logger.warning(f"âŒ ä»£ç†ä¸å¯ç”¨: {proxy}")
        return False, 15.0
    
    async def update_proxy_pool(self):
        self.logger.info("ğŸ”„ æ›´æ–°ä»£ç†æ± ...")
        raw_proxies = await self.fetch_proxies()
        valid_proxies = []
        
        # å¹¶è¡ŒéªŒè¯ä»£ç† (é™åˆ¶ä¸º150ä¸ª)
        tasks = [self.validate_proxy(proxy) for proxy in raw_proxies[:150]]
        results = await asyncio.gather(*tasks)
        
        for i, (is_valid, speed) in enumerate(results):
            proxy = raw_proxies[i]
            if is_valid:
                # è·³è¿‡æœ€è¿‘å¤±è´¥çš„ä»£ç†
                if proxy in self.failed_proxies:
                    self.failed_proxies.discard(proxy)
                    
                valid_proxies.append(proxy)
                # æ ¹æ®é€Ÿåº¦è¯„åˆ† (1-10)
                self.proxy_score[proxy] = max(1, min(10, int(10 - speed * 2)))
                self.logger.info(f"âœ… ä»£ç†å¯ç”¨: {proxy} | é€Ÿåº¦: {speed:.2f}s | è¯„åˆ†: {self.proxy_score[proxy]}")
            else:
                # æ ‡è®°å¤±è´¥ä»£ç†
                self.failed_proxies.add(proxy)
        
        self.proxy_pool = valid_proxies
        self.last_refresh = datetime.now()
        self.logger.info(f"ä»£ç†æ± æ›´æ–°å®Œæˆ. å¯ç”¨ä»£ç†: {len(self.proxy_pool)}")
    
    async def get_best_proxy(self):
        """è·å–æœ€ä½³ä»£ç†ï¼Œè‡ªåŠ¨åˆ·æ–°æ± ï¼Œè·³è¿‡å¤±è´¥ä»£ç†"""
        # æ¯10åˆ†é’Ÿåˆ·æ–°ä¸€æ¬¡ä»£ç†æ± 
        if (datetime.now() - self.last_refresh) > timedelta(minutes=10) or not self.proxy_pool:
            await self.update_proxy_pool()
        
        if not self.proxy_pool:
            # æ·»åŠ å›é€€æœºåˆ¶ï¼šå½“æ²¡æœ‰ä»£ç†æ—¶å°è¯•ç›´æ¥è¿æ¥
            self.logger.warning("âš ï¸ æ²¡æœ‰å¯ç”¨ä»£ç†ï¼Œå°è¯•ç›´æ¥è¿æ¥...")
            return None
        
        # æ ¹æ®è¯„åˆ†åŠ æƒéšæœºé€‰æ‹©ï¼Œè·³è¿‡æœ€è¿‘å¤±è´¥çš„ä»£ç†
        weighted_pool = []
        for proxy in self.proxy_pool:
            if proxy in self.failed_proxies:
                continue
                
            weight = self.proxy_score.get(proxy, 5)
            weighted_pool.extend([proxy] * weight)
        
        if not weighted_pool:
            self.logger.warning("âš ï¸ æ²¡æœ‰é«˜è´¨é‡ä»£ç†ï¼Œä½¿ç”¨ä½è´¨é‡ä»£ç†...")
            # å¦‚æœæ²¡æœ‰é«˜è´¨é‡ä»£ç†ï¼Œå°è¯•ä½¿ç”¨ä»»ä½•å¯ç”¨ä»£ç†
            for proxy in self.proxy_pool:
                weight = max(1, self.proxy_score.get(proxy, 1))
                weighted_pool.extend([proxy] * weight)
        
        return random.choice(weighted_pool) if weighted_pool else None
    
    def report_proxy_failure(self, proxy):
        """ä»£ç†å¤±è´¥å¤„ç†"""
        # æ·»åŠ åˆ°å¤±è´¥ä»£ç†åˆ—è¡¨
        self.failed_proxies.add(proxy)
        
        if proxy in self.proxy_score:
            self.proxy_score[proxy] = max(1, self.proxy_score[proxy] - 3)
            self.logger.warning(f"âš ï¸ ä»£ç†é™çº§: {proxy} | æ–°è¯„åˆ†: {self.proxy_score[proxy]}")
